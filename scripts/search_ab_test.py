#!/usr/bin/env python3
"""Phase 4.1 ‚Äî Reranker A/B ÌÖåÏä§Ìä∏ + Í≤ÄÏÉâ Í∞ÄÏ§ëÏπò ÌäúÎãù Î≤§ÏπòÎßàÌÅ¨.

ÌÖåÏä§Ìä∏ ÏøºÎ¶¨ ÏÖãÏúºÎ°ú reranker ON/OFF + ÎèôÏ†Å Í∞ÄÏ§ëÏπò ON/OFF ÎπÑÍµê.

Usage:
    python scripts/search_ab_test.py
    python scripts/search_ab_test.py --reranker-only
    python scripts/search_ab_test.py --weights-only
"""
from __future__ import annotations

import argparse
import json
import os
import sys
import time
from dataclasses import dataclass
from pathlib import Path

os.environ.setdefault("PYTHONIOENCODING", "utf-8")
sys.stdout.reconfigure(encoding="utf-8", line_buffering=True)
sys.stderr.reconfigure(encoding="utf-8")

PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "src"))

# --- Test queries with expected top results ---
TEST_QUERIES = [
    {
        "query": "Î≤†Ìä∏ÎÇ® ÌôîÏû•Ìíà Zalo ÎßàÏºÄÌåÖ Ï†ÑÎûµ",
        "type": "factual",
        "expected_top": ["vietnam-beauty", "Î≤†Ìä∏ÎÇ® ÌôîÏû•Ìíà ÏÇ¨ÏóÖ Ï†ÑÎûµ", "Zalo"],
        "description": "ÏÇ¨Ïã§ Í∏∞Î∞ò - ÌäπÏ†ï ÌîÑÎ°úÏ†ùÌä∏+Í∏∞Ïà† Ï°∞Ìï©",
    },
    {
        "query": "MAIOSS Î≥¥Ïïà Ïä§Ï∫êÎÑà ÏïÑÌÇ§ÌÖçÏ≤ò",
        "type": "factual",
        "expected_top": ["maioss", "MAIOSS"],
        "description": "ÏÇ¨Ïã§ Í∏∞Î∞ò - ÌäπÏ†ï ÌîÑÎ°úÏ†ùÌä∏",
    },
    {
        "query": "AI ÏàòÏùµÌôî Î™®Îç∏ ÎπÑÍµê",
        "type": "exploratory",
        "expected_top": ["AI ÏàòÏùµÌôî", "ÏàòÏùµÌôî", "monetization"],
        "description": "ÌÉêÏÉâ - ÎÑìÏùÄ Ï£ºÏ†ú",
    },
    {
        "query": "ÏÇºÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅ ÎØ∏ÌåÖÏóêÏÑú ÎÖºÏùòÎêú Í∏∞Ïà† Í≥ºÏ†ú",
        "type": "relational",
        "expected_top": ["ÏÇºÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅ", "ÎØ∏ÌåÖ", "POC"],
        "description": "Í¥ÄÍ≥Ñ Í∏∞Î∞ò - Ïù¥Î≤§Ìä∏+Ï°∞ÏßÅ",
    },
    {
        "query": "Obsidian ÌîåÎü¨Í∑∏Ïù∏ Í∞úÎ∞ú Í≥ÑÌöç",
        "type": "factual",
        "expected_top": ["maisecondbrain", "Obsidian", "ÌîåÎü¨Í∑∏Ïù∏"],
        "description": "ÏÇ¨Ïã§ Í∏∞Î∞ò - Î°úÎìúÎßµ",
    },
    {
        "query": "ÍπÄÏ≤†Ïàò Îß§ÎãàÏ†Ä Í¥ÄÎ†® ÌîÑÎ°úÏ†ùÌä∏",
        "type": "relational",
        "expected_top": ["ÍπÄÏ≤†Ïàò", "ÍπÄÏ≤†Ïàò Îß§ÎãàÏ†Ä"],
        "description": "Í¥ÄÍ≥Ñ Í∏∞Î∞ò - Ïù∏Î¨º Ï§ëÏã¨",
    },
    {
        "query": "GraphRAG ÌïòÏù¥Î∏åÎ¶¨Îìú Í≤ÄÏÉâ Íµ¨ÌòÑ Î∞©Î≤ï",
        "type": "factual",
        "expected_top": ["GraphRAG", "hybrid_search", "ÌïòÏù¥Î∏åÎ¶¨Îìú"],
        "description": "ÏÇ¨Ïã§ Í∏∞Î∞ò - Í∏∞Ïà† Íµ¨ÌòÑ",
    },
    {
        "query": "BOT Suite ÏãúÎÑàÏßÄ Ï†ÑÎûµ",
        "type": "exploratory",
        "expected_top": ["BOTALKS", "BOTCON", "BOT Suite"],
        "description": "ÌÉêÏÉâ - ÌÅ¨Î°úÏä§ ÌîÑÎ°úÏ†ùÌä∏",
    },
]


@dataclass
class SearchConfig:
    name: str
    reranker: bool
    dynamic_weights: bool


def _relevance_score(results: list[dict], expected: list[str]) -> float:
    """Í≤∞Í≥ºÏùò Í¥ÄÎ†®ÎèÑ Ï†êÏàò (0~1). top-5Ïóê expected ÌÇ§ÏõåÎìúÍ∞Ä Ìè¨Ìï®Îêú ÎπÑÏú®."""
    if not expected:
        return 0.0
    top_names = " ".join(r.get("name", "") + " " + r.get("key", "") for r in results[:5]).lower()
    hits = sum(1 for e in expected if e.lower() in top_names)
    return hits / len(expected)


def run_search(query: str, reranker: bool, dynamic_weights: bool,
               G, embeddings, notes_content, embed_fn, top_k: int = 5):
    """Run a single search with specified config."""
    from mnemo.hybrid_search import hybrid_search

    # set reranker env
    os.environ["MNEMO_USE_RERANKER"] = "true" if reranker else "false"

    # determine weights
    if dynamic_weights:
        kw, vw, gw = classify_and_weight(query)
    else:
        kw, vw, gw = 0.5, 0.3, 0.2

    query_embedding = embed_fn(query) if embed_fn else None

    t0 = time.time()
    results = hybrid_search(
        query=query, G=G, embeddings=embeddings,
        notes_content=notes_content, query_embedding=query_embedding,
        top_k=top_k,
        keyword_weight=kw, vector_weight=vw, graph_weight=gw,
    )
    elapsed = time.time() - t0

    return [
        {
            "key": r.key,
            "name": r.name,
            "score": round(r.score, 4),
            "keyword_score": round(r.keyword_score, 4),
            "vector_score": round(r.vector_score, 4),
            "graph_score": round(r.graph_score, 4),
            "entity_type": r.entity_type,
        }
        for r in results
    ], elapsed


def classify_and_weight(query: str) -> tuple[float, float, float]:
    """ÏøºÎ¶¨ ÌÉÄÏûÖ ÏûêÎèô Î∂ÑÎ•ò + ÎèôÏ†Å Í∞ÄÏ§ëÏπò Î∞òÌôò."""
    from mnemo.query_classifier import classify_query, get_weights
    qtype = classify_query(query)
    return get_weights(qtype)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--reranker-only", action="store_true")
    parser.add_argument("--weights-only", action="store_true")
    args = parser.parse_args()

    # load context once
    sys.path.insert(0, str(PROJECT_ROOT / "scripts"))
    from integrated_search import _load_vault_context
    print("Loading vault context...", flush=True)
    G, embeddings, notes_content, embed_fn = _load_vault_context()
    if G is None:
        print("ERROR: Could not load graph"); sys.exit(1)
    print(f"  Graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges")

    configs = []
    if args.reranker_only:
        configs = [
            SearchConfig("baseline", False, False),
            SearchConfig("reranker", True, False),
        ]
    elif args.weights_only:
        configs = [
            SearchConfig("fixed_weights", False, False),
            SearchConfig("dynamic_weights", False, True),
        ]
    else:
        configs = [
            SearchConfig("A: baseline", False, False),
            SearchConfig("B: reranker", True, False),
            SearchConfig("C: dynamic_wt", False, True),
            SearchConfig("D: reranker+dyn", True, True),
        ]

    print(f"\n{'='*70}")
    print(f"Phase 4 A/B Test ‚Äî {len(TEST_QUERIES)} queries √ó {len(configs)} configs")
    print(f"{'='*70}\n")

    summary = {cfg.name: {"total_relevance": 0.0, "total_time": 0.0} for cfg in configs}

    for i, tq in enumerate(TEST_QUERIES, 1):
        q = tq["query"]
        qtype = tq["type"]
        expected = tq["expected_top"]
        print(f"[{i}/{len(TEST_QUERIES)}] {tq['description']}")
        print(f"  Query: {q}")
        print(f"  Type: {qtype} | Expected: {expected}")

        for cfg in configs:
            results, elapsed = run_search(
                q, cfg.reranker, cfg.dynamic_weights,
                G, embeddings, notes_content, embed_fn
            )
            rel = _relevance_score(results, expected)
            summary[cfg.name]["total_relevance"] += rel
            summary[cfg.name]["total_time"] += elapsed

            top3 = [f"{r['name'][:25]}({r['score']:.3f})" for r in results[:3]]
            marker = "‚úÖ" if rel >= 0.5 else "‚ö†Ô∏è" if rel > 0 else "‚ùå"
            print(f"  {cfg.name:20s} | rel={rel:.2f} {marker} | {elapsed:.3f}s | {', '.join(top3)}")

        print()

    # Final summary
    print(f"{'='*70}")
    print("SUMMARY")
    print(f"{'='*70}")
    n = len(TEST_QUERIES)
    for cfg in configs:
        s = summary[cfg.name]
        avg_rel = s["total_relevance"] / n
        avg_time = s["total_time"] / n
        print(f"  {cfg.name:20s} | avg_relevance={avg_rel:.3f} | avg_time={avg_time:.3f}s")

    # winner
    best = max(summary.items(), key=lambda x: x[1]["total_relevance"])
    print(f"\nüèÜ Best config: {best[0]} (avg rel: {best[1]['total_relevance']/n:.3f})")

    # Save results
    out = PROJECT_ROOT / ".mnemo" / "ab_test_results.json"
    out.write_text(json.dumps(summary, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"Results saved: {out}")


if __name__ == "__main__":
    main()
